{
  "metadata": {
    "anaconda-cloud": {},
    "language_info": {
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "version": "3.6.1",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Import all dependencies\n",
        "import numpy as np\n",
        "import random as rd\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "from scipy.stats import norm\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Simulate Rankings of Relevance for E and P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def simulate_rankings():\n",
        "    ''' This method creates all possible combinations of rankings for P and E.\n",
        "\n",
        "        @Output: a list of tuples. A tuple has 2 integer arrays (one ranking for P and one for E).\n",
        "    '''\n",
        "    P = [[0, 0, 0], [1, 0, 0], [1, 1, 0], [1, 1, 1],\n",
        "         [1, 0, 1], [0, 1, 1], [0, 0, 1], [0, 1, 0]]\n",
        "    E = [[0, 0, 0], [1, 0, 0], [1, 1, 0], [1, 1, 1],\n",
        "         [1, 0, 1], [0, 1, 1], [0, 0, 1], [0, 1, 0]]\n",
        "    rankings = []\n",
        "\n",
        "    for p in P:\n",
        "        for e in E:\n",
        "            tup1 = (p, e)\n",
        "            rankings.append(tup1)\n",
        "\n",
        "    print(rankings)\n",
        "    return rankings\n",
        "\n",
        "\n",
        "rankings = simulate_rankings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Calculate the $\\Delta$ measure\n",
        "\n",
        "We calculate the Expected reciprocal rank of a ranking using the guy \n",
        "\n",
        "ERR := $\\sum_{r = 1}^n \\frac{1}{r}$ P(user stops at position r)\n",
        "\n",
        "where n is the number of documents in the ranking and\n",
        "\n",
        "$P = \\prod_{i=1}^{r-1} (1-R_i)R_r$\n",
        "\n",
        "where $R_i = \\frac{2^g - 1}{2^{g_{max}}}$ where $g_i$ is the grade of the i-th document and $g_{max}$ is the the maximum relevance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def calculate_ERR(ranking):\n",
        "    ''' This method calculates the ERR of a ranking. A ranking is a one dimensional interger list with length 3.\n",
        "\n",
        "        @Input: a one dimensional list of length 3 with zeros and ones.\n",
        "\n",
        "        @Output: a double (ERR score).\n",
        "    '''\n",
        "    ERR = 0\n",
        "    for r in range(len(ranking)):\n",
        "        prob_to_stop_at_r = ranking[r]/(r+1)/2\n",
        "        for i in range(r):\n",
        "            prob_to_stop_at_r *= 1 - ranking[i]/2\n",
        "\n",
        "        ERR += prob_to_stop_at_r\n",
        "\n",
        "    return ERR\n",
        "\n",
        "\n",
        "calculate_ERR(rankings[5][1])\n",
        "# rankings[5][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The buckets are made such that group 1 contains all pairs for which 0.05 < $\\Delta$ measure \u2264 0.1, group 2 all pairs for which 0.1 < $\\Delta$measure \u2264 0.2, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def calculate_Dmeasures(rankings):\n",
        "    ''' This method calculates the difference in ERR between the two rankings of every tuple, \n",
        "        for all tuples of rankings.\n",
        "        Since we need to store the results in 10 different buckets we use a dictionary with keys 10 integers \n",
        "        from 0 to 9 with step 1 and values the list of our measurements.\n",
        "\n",
        "        @Input: a list of tuples.\n",
        "\n",
        "        @Output: A dictionary with keys integers from 0 to 9 and values 10 lists of doubles. \n",
        "\n",
        "    '''\n",
        "    measures = {k: [] for k in range(10)}\n",
        "\n",
        "    # The indices in measures are now the indices of the pair tuples\n",
        "    for index, r in enumerate(rankings):\n",
        "        ERR_P = calculate_ERR(r[0])\n",
        "        ERR_E = calculate_ERR(r[1])\n",
        "\n",
        "        d_measure = ERR_E - ERR_P\n",
        "        if d_measure >= 0.05 and d_measure <= 0.95:\n",
        "            measures[int(d_measure * 10)].append(index)\n",
        "\n",
        "    return measures\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(calculate_Dmeasures(rankings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Implement Team-Draft Interleaving and Probabilistic Intearleaving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from IPython.core.debugger import set_trace\n",
        "\n",
        "\n",
        "def convert_lists_to_labeled(list_a, list_b):\n",
        "    considered_lists = [deepcopy(list_a), deepcopy(list_b)]\n",
        "    label_results = []\n",
        "    # set_trace()\n",
        "\n",
        "    for doc_list in considered_lists:\n",
        "        possible_labels = rd.sample(range(1, 20), 3)\n",
        "        label_results.append(list(zip(doc_list, possible_labels)))\n",
        "\n",
        "    return label_results\n",
        "\n",
        "\n",
        "def team_draft_interleaving(list_a, list_b):\n",
        "    ''' Team draft interleaving is performed by throwing a coin. If its heads we put in the interleaved list\n",
        "        the first element of list A that is not already in,\n",
        "        if its tails we put in the interleaved list the first element of list B that is not already in.\n",
        "\n",
        "        @Input: 2 lists of integers of length 3.\n",
        "\n",
        "        @Output: an interleaved list of tuples of length 3. Each tuple contain an integer (document ID) and a 0 or 1 \n",
        "        depending of which list it came from.\n",
        "    '''\n",
        "\n",
        "    unique_doc_list = []\n",
        "    interleaved_list = []\n",
        "    counter = 0\n",
        "    list_a_labeled, list_b_labeled = convert_lists_to_labeled(\n",
        "        deepcopy(list_a), deepcopy(list_b))\n",
        "\n",
        "    while counter < 3:\n",
        "        coin_toss = rd.random()\n",
        "\n",
        "        if(coin_toss > 0.5):\n",
        "            put_first_available_url_in_interleaved(\n",
        "                list_a_labeled, 0, interleaved_list, unique_doc_list)\n",
        "            counter += 1\n",
        "\n",
        "            if(counter == 3):\n",
        "                return interleaved_list\n",
        "\n",
        "            put_first_available_url_in_interleaved(\n",
        "                list_b_labeled, 1, interleaved_list, unique_doc_list)\n",
        "            counter += 1\n",
        "        else:\n",
        "            put_first_available_url_in_interleaved(\n",
        "                list_b_labeled, 1, interleaved_list, unique_doc_list)\n",
        "            counter += 1\n",
        "            if(counter == 3):\n",
        "                return interleaved_list\n",
        "\n",
        "            put_first_available_url_in_interleaved(\n",
        "                list_a_labeled, 0, interleaved_list, unique_doc_list)\n",
        "            counter += 1\n",
        "\n",
        "    print(\"List is:\", interleaved_list)\n",
        "    return interleaved_list\n",
        "\n",
        "\n",
        "def put_first_available_url_in_interleaved(a_list, index_list, interleaved_list, unique_doc_list):\n",
        "    ''' Helper method that creates a tuple with and integer (document ID) and \n",
        "        a 0 or 1 which indicates from which list it came from and adds it in the interleaved list. \n",
        "\n",
        "        @Input: - a list of intergers of length 3.\n",
        "                - a 0 or 1 indicator of the list.\n",
        "                - the interleaved list with all tuples of rnakings.\n",
        "\n",
        "    '''\n",
        "\n",
        "    for i in a_list:\n",
        "        if i[1] not in unique_doc_list:\n",
        "            interleaved_list.append((i[0], index_list))\n",
        "            unique_doc_list.append(i[1])\n",
        "            return\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "team_draft_interleaving(rankings[0][0], rankings[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\textbf{Probabilistic Intearleaving}$ is performed similarly but instead of choosing the first document we chose the documents from the lists probabilisticly where the probabilities of the documents are produced from 2 softmax functions\n",
        "(one for every list)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def probabilistic_interleaving(list_a, list_b):\n",
        "    ''' Probabilistic interleaving perfromed by throwing coins, if its heads we use the softmax function of list A\n",
        "        to choose randomly (with higher probability the more relevant rankings)\n",
        "        else we use the softmax function of list B. \n",
        "        After we put the choosen doc we remove it from both lists A and B softmaxes.\n",
        "        We do this until the interleaved list is full.\n",
        "        The interleaved list contains tuples with the document ID and a 0 or 1 considering the list that it came from.\n",
        "\n",
        "        @Input: 2 lists of integers of length 3.\n",
        "\n",
        "        @Output: an interleaved list of tuples of length 3. Each tuple contain an integer (document ID) and a 0 or 1 \n",
        "        depending of which list it came from.\n",
        "\n",
        "    '''\n",
        "    interleaved_list = [ ]\n",
        "    counter = 0\n",
        "    list_a, list_b = deepcopy(list_a), deepcopy(list_b)\n",
        "    while counter < 3:\n",
        "        coin_toss = rd.random()\n",
        "        if len(list_a+list_b) == 0:\n",
        "            raise InvalidArgumentError\n",
        "        if(coin_toss > 0.5):\n",
        "            if len(list_a) == 0:\n",
        "                continue\n",
        "            probs = softmax(list_a)\n",
        "            chosen = np.random.choice(list_a, 1, p = probs)\n",
        "            tup = (int(chosen), 0)\n",
        "            interleaved_list.append(tup)\n",
        "               \n",
        "        else:\n",
        "            if len(list_b) == 0:\n",
        "                continue\n",
        "            probs = softmax(list_b)\n",
        "            chosen = np.random.choice(list_b, 1, p = probs)\n",
        "            tup = (int(chosen), 1)\n",
        "            interleaved_list.append(tup)\n",
        "      \n",
        "        counter += 1  \n",
        "        if chosen in list_a: \n",
        "            list_a.remove(chosen)\n",
        "            \n",
        "        if chosen in list_b: \n",
        "            list_b.remove(chosen)\n",
        "            \n",
        "    return interleaved_list\n",
        "    \n",
        "\n",
        "\n",
        "def softmax(a_list, tau=3):\n",
        "    ''' Helper method that calculates the probabilities of every document in the given list\n",
        "        using the softmax function in a vectorised from.\n",
        "\n",
        "        @Input: list of intergers of length 3 (rankings).\n",
        "\n",
        "        @Output: a vector with probabilities for every document.\n",
        "    '''\n",
        "    rankings = []\n",
        "    for i,doc in enumerate(a_list):\n",
        "        rankings.append(i+1)\n",
        "    \n",
        "    numerators = 1 / np.power(rankings, tau)\n",
        " \n",
        "    denominator = numerators.sum()\n",
        "   \n",
        "    return numerators / denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Implement Click-based models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class YandexData():\n",
        "    \"\"\"\n",
        "        The structure of lookup table:\n",
        "        {\n",
        "            'q_id':{\n",
        "                'sessions':[\n",
        "                {\n",
        "                    'url_ids':[],\n",
        "                    'clicks':[]\n",
        "                }\n",
        "                ]\n",
        "                'docs':set([])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        @Properties:        \n",
        "            - `q_id` is the id of the different queries, key to object:\n",
        "                - `session` is a list of the sessions of this particular `q_id`\n",
        "                    each `session` have a list of `url_ids` and a `clicks` list, \n",
        "                    the url in `clicks` must be in the `url_ids`\n",
        "                - `docs` is the union of all urls returned by the system per query\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        CUTOFF = 3\n",
        "        queries_lookup = {}\n",
        "\n",
        "        # Lambda functions\n",
        "        def new_item(): return {'sessions': [], 'docs': set()}\n",
        "\n",
        "        def turn2int(x): return [int(i) for i in x]\n",
        "\n",
        "        with open(self.path, 'r') as f:\n",
        "            click = []\n",
        "            last_q = None\n",
        "\n",
        "            for line in f.readlines():\n",
        "                vals = re.split(r'\\t+', line.rstrip())\n",
        "\n",
        "                # If line is a query\n",
        "                if vals[2] == 'Q':\n",
        "                    current_q = vals[3]\n",
        "\n",
        "                    # Get the relevant URLs\n",
        "                    cutoff_urls = turn2int(vals[5:5 + CUTOFF])\n",
        "\n",
        "                    if current_q not in queries_lookup.keys():\n",
        "                        it = new_item()\n",
        "                    else:\n",
        "                        it = queries_lookup[current_q]\n",
        "\n",
        "                    # Append documents for this query and add session\n",
        "                    it['docs'] = it['docs'].union(cutoff_urls)\n",
        "                    it['sessions'].append({'urls': cutoff_urls, 'clicks': []})\n",
        "\n",
        "                    queries_lookup[current_q] = it\n",
        "                    last_q = current_q\n",
        "\n",
        "                # Else if line is a click\n",
        "                elif vals[2] == 'C':\n",
        "                    # If the document has been found in the active query, add\n",
        "                    # the document to the selection of clicks\n",
        "                    if int(vals[3]) in queries_lookup[last_q]['sessions'][-1]['urls']:\n",
        "                        queries_lookup[last_q][\n",
        "                            'sessions'][-1]['clicks'].append(int(vals[3]))\n",
        "\n",
        "        self.queries_lookup = queries_lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ClickModel(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_probs(self, rankings):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def is_click(self, rankings, epsilon=0.1):\n",
        "        \"\"\"\n",
        "            simulate the click, return a boolean list of the same length as `rankings`, True means clicked\n",
        "        \"\"\"\n",
        "        probs = self.get_probs(rankings, epsilon)\n",
        "\n",
        "        def click_fn(p): return rd.uniform(0, 1) < p\n",
        "        return list(map(click_fn, probs))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class PBM(ClickModel):\n",
        "    def __init__(self):\n",
        "        super(PBM, self).__init__()\n",
        "        self.alpha_uq = {}\n",
        "        self.gamma_r = [rd.uniform(0, 1) for _ in range(CUTOFF)]\n",
        "\n",
        "    def train(self, data, T=20, load=False):\n",
        "        \"\"\" Trains the parameters of the model according to the data.\n",
        "\n",
        "            @Input:\n",
        "                - data: YandexData object, Yandex data\n",
        "                - T: integer, time steps of the training loop\n",
        "                - load: boolean, whether trained gamma is used (we dont need trained alpha because during inference they are replaced by epsilon)\n",
        "        \"\"\"\n",
        "\n",
        "        if load:\n",
        "            self.gamma_r = [0.9998564405092062,\n",
        "                            0.48278049975990095, 0.3335993103977007]\n",
        "            return\n",
        "\n",
        "        self._init_alpha(data)\n",
        "\n",
        "        for _ in range(T):\n",
        "            self._update_alpha(data)\n",
        "            self._update_gamma(data)\n",
        "\n",
        "    def _init_alpha(self, data):\n",
        "        \"\"\" Initializes alpha on the model to be a dictionary only where (query, doc) have a value\n",
        "\n",
        "            @Input:\n",
        "                - data: YandexData object\n",
        "        \"\"\"\n",
        "        for q, it in data.queries_lookup.items():\n",
        "            for doc in it['docs']:\n",
        "                self.alpha_uq[(q, doc)] = rd.uniform(0, 1)\n",
        "\n",
        "    def _update_alpha(self, data):\n",
        "        \"\"\" Performs an update of alpha in EM\n",
        "\n",
        "            @Input:\n",
        "                - data: YandexData object\n",
        "        \"\"\"\n",
        "\n",
        "        # Get all the sessions and docs belonging to a query.\n",
        "        ql = data.queries_lookup\n",
        "        new_alpha_uq = deepcopy(self.alpha_uq)\n",
        "\n",
        "        # Loop through all query-doc combos\n",
        "        for (q, u), alpha in self.alpha_uq.items():\n",
        "            count = 2\n",
        "            contribution_sum = 1\n",
        "\n",
        "            for sess in ql[q]['sessions']:\n",
        "                if u not in sess['urls']:\n",
        "                    continue\n",
        "\n",
        "                count += 1\n",
        "                if u in sess['clicks']:\n",
        "                    contribution_sum += 1\n",
        "                else:\n",
        "                    ind = sess['urls'].index(u)\n",
        "                    contribution_sum += (1 - self.gamma_r[ind]) * \\\n",
        "                        alpha / (1 - self.gamma_r[ind] * alpha)\n",
        "\n",
        "            new_alpha_uq[(q, u)] = contribution_sum / count\n",
        "\n",
        "        self.alpha_uq = new_alpha_uq\n",
        "\n",
        "    def _update_gamma(self, data):\n",
        "        \"\"\" Performs an update of gamma in EM\n",
        "\n",
        "            @Input:\n",
        "                - data: YandexData object\n",
        "        \"\"\"\n",
        "\n",
        "        ql = data.queries_lookup\n",
        "        sess_num = 0\n",
        "        contrib_sum = [0] * CUTOFF\n",
        "        for q, item in ql.items():\n",
        "            sess_num += len(item['sessions'])\n",
        "            for sess in item['sessions']:\n",
        "                for i, u in enumerate(sess['urls']):\n",
        "                    if u in sess['clicks']:\n",
        "                        contrib_sum[i] += 1\n",
        "                    else:\n",
        "                        contrib_sum[i] += self.gamma_r[i] * (1 - self.alpha_uq[q, u]) / (\n",
        "                            1 - self.gamma_r[i] * self.alpha_uq[q, u])\n",
        "\n",
        "        self.gamma_r = [i / sess_num for i in contrib_sum]\n",
        "        return\n",
        "\n",
        "    def get_probs(self, rankings, epsilon=0.1):\n",
        "        \"\"\"\n",
        "            assume `rankings` are list of relevance labels\n",
        "            use `epsilon` to substitute alpha, typical value is 0.1\n",
        "        \"\"\"\n",
        "        def prob_fn(args): return self.gamma_r[\n",
        "            args[0]] * (1 - epsilon if args[1] == 1 else epsilon)\n",
        "        return list(map(prob_fn, enumerate(rankings)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "CUTOFF = 3\n",
        "\n",
        "\n",
        "class RCM(ClickModel):\n",
        "    \"\"\"\n",
        "        Random clicking model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RCM, self).__init__()\n",
        "        self.gamma = [0] * 3\n",
        "\n",
        "    def train(self, data, load=True):\n",
        "        \"\"\"\n",
        "            get the \\rho paramter for random clicking by calculating the fraction of clicked urls among all returned results\n",
        "        \"\"\"\n",
        "        if load:\n",
        "            self.rho = 0.2802838475726031\n",
        "            return\n",
        "        sess_num = 0\n",
        "        cli_num = 0\n",
        "        for q, it in data.queries_lookup.items():\n",
        "            for sess in it['sessions']:\n",
        "                cli_num += len(sess['clicks'])\n",
        "            sess_num += len(it['sessions'])\n",
        "        self.rho = cli_num / sess_num / CUTOFF\n",
        "        return\n",
        "\n",
        "    def get_probs(self, rankings, epsilon=None):\n",
        "        \"\"\"\n",
        "            return \\rho list regardless\n",
        "        \"\"\"\n",
        "        return [self.rho] * len(rankings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Simulate Interleaving Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Import the appropriate data\n",
        "yd = YandexData('./YandexRelPredChallenge.txt')\n",
        "\n",
        "# Training the PBM and RCM model\n",
        "model_PBM = PBM()\n",
        "model_PBM.train(yd, 100, True)\n",
        "model_RCM = RCM()\n",
        "model_RCM.train(yd, True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for i, bin_val in calculate_Dmeasures(rankings).items():\n",
        "    for pair_idx in bin_val:\n",
        "        couple = rankings[pair_idx]\n",
        "        team_draft_interleaving(couple[0], couple[1])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def simulate_experiment(rankingA, rankingB, model, interleave_fn=team_draft_interleaving, k=100):\n",
        "    E_wins = 0\n",
        "    P_wins = 0\n",
        "\n",
        "    for i in range(k):\n",
        "        E_clicks = 0\n",
        "        P_clicks = 0\n",
        "        new_results_w_models = interleave_fn(rankingA, rankingB)\n",
        "        new_results_relevance = [i[0] for i in new_results_w_models]\n",
        "        ranker_clicked = [i[1] for i in new_results_w_models]\n",
        "        clicks = model.is_click(rankings=new_results_relevance, epsilon=0.1)\n",
        "\n",
        "        for index, click in enumerate(clicks):\n",
        "            if click:\n",
        "                if ranker_clicked[index] == 1:\n",
        "                    E_clicks += 1\n",
        "                else:\n",
        "                    P_clicks += 1\n",
        "\n",
        "        if E_clicks > P_clicks:\n",
        "            E_wins += 1\n",
        "        elif P_clicks > E_clicks:\n",
        "            P_wins += 1\n",
        "\n",
        "    return (E_wins + 1) / (E_wins + P_wins + 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Estimate sample size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def calc_sample_size(p_val, alpha=0.05, beta=0.10, p_null=0.5):\n",
        "    z = norm.ppf(1-alpha)*math.sqrt(p_null * (1 - p_null)) + \\\n",
        "        norm.ppf(1-beta) * math.sqrt(p_val * (1-p_val))\n",
        "\n",
        "    if p_val == p_null:\n",
        "        return math.inf\n",
        "\n",
        "    if z == 0.0:\n",
        "        return -1\n",
        "\n",
        "    return ((z/(abs(p_val-p_null)))**2) + 1/abs(p_val-p_null)\n",
        "\n",
        "\n",
        "def calc_sample_size_for_bins(interleave_fn=team_draft_interleaving, model=model_PBM, rankings=rankings):\n",
        "    bins = calculate_Dmeasures(rankings)\n",
        "    bin_vals = list(bins.keys())\n",
        "    table = pd.DataFrame(index=bin_vals,columns=['minimum', 'mean', 'maximum'])    \n",
        "\n",
        "    for bin_key, bin_el in bins.items():\n",
        "        minimum, mean, maximum = calc_sample_size_for_bin(bin_el, interleave_fn, model)\n",
        "        table.loc[bin_key]['minimum'] = minimum\n",
        "        table.loc[bin_key]['mean'] = mean\n",
        "        table.loc[bin_key]['maximum'] = maximum\n",
        "    \n",
        "    return table\n",
        "\n",
        "\n",
        "def calc_sample_size_for_bin(binned_el, interleave_fn, model):\n",
        "    result = []\n",
        "\n",
        "    for pair in binned_el:\n",
        "        pairE = rankings[pair][0]\n",
        "        pairP = rankings[pair][1]\n",
        "        proportion_E_win = simulate_experiment(pairE, pairP, model, interleave_fn)\n",
        "        sample_size = calc_sample_size(proportion_E_win)\n",
        "\n",
        "        if sample_size >= 0:\n",
        "            result.append(calc_sample_size(proportion_E_win))\n",
        "\n",
        "    if len(binned_el) > 0:\n",
        "        maximum = np.max(result)\n",
        "        minimum = np.min(result)\n",
        "        mean = np.mean(result)\n",
        "\n",
        "        return (minimum, mean, maximum)\n",
        "    \n",
        "    return math.inf, math.inf, math.inf\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "int_methods = [team_draft_interleaving, probabilistic_interleaving]\n",
        "\n",
        "def run_all_setups(models=[model_PBM, model_RCM], methods=[team_draft_interleaving, probabilistic_interleaving]):\n",
        "    for model in models:\n",
        "        for method in methods:\n",
        "            table_setup = calc_sample_size_for_bins(interleave_fn=method, model=model)\n",
        "            # TODO: What should we do?\n",
        "            print(table_setup)\n",
        "            table_setup.plot.bar()\n",
        "\n",
        "# In[73]:\n",
        "\n",
        "\n",
        "list_a = [3, 1, 5]\n",
        "list_b = [1, 2, 10]\n",
        "\n",
        "probs = softmax(list_a)\n",
        "print(probs)\n",
        "res = np.random.choice(list_a, 1, p = probs)\n",
        "print(res)\n",
        "interleaved = probabilistic_interleaving(list_a, list_b)\n",
        "print(interleaved)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "run_all_setups()\n",
        "\n"
      ]
    }
  ],
  "nbformat_minor": 1
}